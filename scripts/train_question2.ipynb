{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from torch import nn\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, load the data:\n",
    "def extract_df_from_jsons(json_path):\n",
    "    with open(json_path, 'r') as fl:\n",
    "        train_dict = json.load(fl)\n",
    "    \n",
    "    car_df = pd.DataFrame()\n",
    "    train_df = pd.DataFrame()\n",
    "    for train_id in train_dict.keys():\n",
    "        train = train_dict[train_id]\n",
    "        train['train_id'] = train_id\n",
    "        for car_id in train['cars']:\n",
    "            train['cars'][car_id]['car_id'] = car_id\n",
    "            train['cars'][car_id]['train_id'] = train_id\n",
    "        car_df = pd.concat([car_df, pd.DataFrame(train['cars']).T])\n",
    "        tmp = train.copy()\n",
    "        tmp.pop('cars')\n",
    "        train_df = pd.concat([train_df, pd.DataFrame(train, index=[train_id])])\n",
    "    train_df = train_df.drop('cars', axis=1)\n",
    "    return car_df, train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df, train_df = extract_df_from_jsons('../data/train_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df2, train_df2 = extract_df_from_jsons('../data/train_dataset2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df = pd.concat([car_df, car_df2])\n",
    "train_df = pd.concat([train_df, train_df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#these ammounts of data is the same as it was in reported in the book\n",
    "len(car_df), len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30000 [00:00<?, ?it/s]f:\\Anaconda\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([71])) that is different to the input size (torch.Size([71, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 30000/30000 [00:15<00:00, 1972.41it/s]\n",
      "f:\\Anaconda\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training Loss 0.0889 for num_wheels concept model.\n",
      "Final validation Loss 0.2106 for num_wheels concept model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:15<00:00, 1910.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training Loss 0.9950 for length concept model.\n",
      "Final validation Loss 1.0057 for length concept model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:14<00:00, 2011.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training Loss 5.6676 for shape concept model.\n",
      "Final validation Loss 4.3261 for shape concept model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:14<00:00, 2043.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training Loss 0.7598 for num_load concept model.\n",
      "Final validation Loss 0.2399 for num_load concept model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:14<00:00, 2064.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training Loss 3.5628 for shape_load concept model.\n",
      "Final validation Loss 4.6613 for shape_load concept model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:14<00:00, 2025.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training Loss 0.8332 for next_crc concept model.\n",
      "Final validation Loss 1.1645 for next_crc concept model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:14<00:00, 2005.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training Loss 0.3999 for next_hex concept model.\n",
      "Final validation Loss 0.0529 for next_hex concept model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:15<00:00, 1884.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training Loss 0.9839 for next_rect concept model.\n",
      "Final validation Loss 0.7516 for next_rect concept model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:15<00:00, 1975.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training Loss 0.9126 for next_tri concept model.\n",
      "Final validation Loss 1.4049 for next_tri concept model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# now defining the models\n",
    "\n",
    "# we will need 12 models\n",
    "# 11 concept models\n",
    "# and 1 meta-network\n",
    "\n",
    "# Since the concept models are basically all the same, I'll define one base model and make whichever changes are needed depending on the concept.\n",
    "\n",
    "class ConceptNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, bind_output=True):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.bind_output = bind_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        hlo = self.relu(x)\n",
    "        x = self.fc2(hlo)\n",
    "        if self.bind_output:\n",
    "            x = self.tanh(x)\n",
    "        return x, hlo\n",
    "\n",
    "class MetaNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size_concepts, num_concepts, hidden_size, list_of_concept_models):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size_concepts * num_concepts, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # self.concept_models = list_of_concept_models\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the networks\n",
    "num_cars = ConceptNetwork(2, 20, 1, False)\n",
    "num_loads = ConceptNetwork(2, 20, 1, False)\n",
    "num_wheels = ConceptNetwork(3, 20, 1, False)\n",
    "length = ConceptNetwork(3, 20, 1)\n",
    "shape = ConceptNetwork(3, 20, 1, False)\n",
    "num_car_loads = ConceptNetwork(3, 20, 1, False)\n",
    "load_shape = ConceptNetwork(3, 20, 1, False)\n",
    "next_crc = ConceptNetwork(3, 20, 1)\n",
    "next_hex = ConceptNetwork(3, 20, 1)\n",
    "next_rec = ConceptNetwork(3, 20, 1)\n",
    "next_tri = ConceptNetwork(3, 20, 1)\n",
    "\n",
    "# num_cars, num_loads\n",
    "list_of_concept_models = [num_wheels, length, shape, num_car_loads, load_shape, next_crc, next_hex, next_rec, next_tri, num_cars, num_loads]\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "list_of_optimizers = [optim.SGD(model.parameters(), lr = 0.01, momentum=0.01) for model in list_of_concept_models]\n",
    "\n",
    "# first I'll train each concept network for 30000 epochs (as in the book)\n",
    "\n",
    "#split the dataset: 36-4\n",
    "train_concept = car_df[car_df.train_id != '3']\n",
    "test_concept = car_df[car_df.train_id == '3']\n",
    "car_ids = train_concept.car_id.values.astype(np.float32)\n",
    "train_ids = train_concept.train_id.values.astype(np.float32)\n",
    "\n",
    "\n",
    "test_car_ids = test_concept.car_id.values.astype(np.float32)\n",
    "test_train_ids = test_concept.train_id.values.astype(np.float32)\n",
    "# Get the activation states of the hidden neurons in the other networks\n",
    "for model, optimizer, feat in zip(list_of_concept_models[:9], list_of_optimizers, train_concept.columns[:9]):\n",
    "    model.train()\n",
    "    train_features = train_concept[feat].values.astype(np.float32)\n",
    "    train_data = torch.tensor(list(zip(train_ids, car_ids, train_features)))\n",
    "    for t in tqdm(range(30000)):\n",
    "        \n",
    "        prediction, _ = model(train_data)\n",
    "        \n",
    "        loss = criterion(prediction, torch.tensor(train_features))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    test_features = test_concept[feat].values.astype(np.float32)\n",
    "    test_data = torch.tensor(list(zip(test_train_ids, test_car_ids, test_features)))\n",
    "    \n",
    "    model.eval()\n",
    "    prediction, _ = model(test_data)\n",
    "    val_loss = criterion(prediction, torch.tensor(test_features))\n",
    "    print(f\"Final training Loss {loss.item():.4f} for {feat} concept model.\")\n",
    "    print(f\"Final validation Loss {val_loss.item():.4f} for {feat} concept model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:14<00:00, 2093.03it/s]\n",
      "100%|██████████| 30000/30000 [00:14<00:00, 2094.88it/s]\n"
     ]
    }
   ],
   "source": [
    "#now training the two other concept models that were left out num_cars and num_loads\n",
    "train_concept_num_car_features = []\n",
    "train_concept_num_loads_features = []\n",
    "for id in train_concept.train_id:\n",
    "    train_concept_num_car_features.append(train_df.num_car.loc[id])\n",
    "    train_concept_num_loads_features.append(train_df.dif_loads.loc[id])\n",
    "\n",
    "\n",
    "test_concept_num_car_features = []\n",
    "test_concept_num_loads_features = []\n",
    "for id in test_concept.train_id:\n",
    "    test_concept_num_car_features.append(train_df.num_car.loc[id])\n",
    "    test_concept_num_loads_features.append(train_df.dif_loads.loc[id])\n",
    "\n",
    "\n",
    "for model, optimizer, feat, test_concept in zip(list_of_concept_models[-2:], list_of_optimizers[-2:], [train_concept_num_car_features, train_concept_num_loads_features], [test_concept_num_car_features, test_concept_num_loads_features]):\n",
    "    model.train()\n",
    "    train_features = np.array(feat, dtype=np.float32)\n",
    "    train_data = torch.tensor(list(zip(train_ids, train_features)))\n",
    "    \n",
    "    for t in tqdm(range(30000)):    \n",
    "        prediction, _ = model(train_data)\n",
    "        \n",
    "        loss = criterion(prediction, torch.tensor(train_features))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:12<00:00, 803.33it/s]\n"
     ]
    }
   ],
   "source": [
    "#now train the meta network that infers wether the train is going east or not\n",
    "\n",
    "#first extracting the outputs from the hidden layers of each concept network\n",
    "hidden_layer_outputs = []\n",
    "for model, optimizer, feat in zip(list_of_concept_models[:9], list_of_optimizers, train_concept.columns[:9]):\n",
    "    model.eval()\n",
    "    train_features = train_concept[feat].values.astype(np.float32)\n",
    "    train_data = torch.tensor(list(zip(train_ids, car_ids, train_features)))\n",
    "    _, hlo = model(train_data)\n",
    "    hidden_layer_outputs.append(hlo)\n",
    "for model, optimizer, feat, test_concept in zip(list_of_concept_models[-2:], list_of_optimizers[-2:], [train_concept_num_car_features, train_concept_num_loads_features], [test_concept_num_car_features, test_concept_num_loads_features]):\n",
    "    model.eval()\n",
    "    train_features = np.array(feat, dtype=np.float32)\n",
    "    train_data = torch.tensor(list(zip(train_ids, train_features)))\n",
    "    _, hlo = model(train_data)\n",
    "    hidden_layer_outputs.append(hlo)\n",
    "\n",
    "\n",
    "direction_label = []\n",
    "for id in train_concept.train_id:\n",
    "    direction_label.append(train_df.east.loc[id])\n",
    "\n",
    "result = torch.cat([hlo.float() for hlo in hidden_layer_outputs], dim=1)\n",
    "\n",
    "metanetwork = MetaNetwork(20, 11, 3, list_of_concept_models)\n",
    "optimizer_meta = optim.SGD(metanetwork.parameters(), lr=0.3, momentum=0.4)\n",
    "for epoch in tqdm(range (10000)):\n",
    "    model.train()\n",
    "    predictions = metanetwork(result)\n",
    "    \n",
    "    loss = criterion(predictions, torch.tensor(direction_label, dtype=torch.float))\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4757],\n",
       "        [0.5106],\n",
       "        [0.5106],\n",
       "        [0.5106],\n",
       "        [0.5106],\n",
       "        [0.5106],\n",
       "        [0.5187],\n",
       "        [0.5348],\n",
       "        [0.5355],\n",
       "        [0.5106],\n",
       "        [0.5452],\n",
       "        [0.5110],\n",
       "        [0.5217],\n",
       "        [0.5231],\n",
       "        [0.5160],\n",
       "        [0.5398],\n",
       "        [0.5459],\n",
       "        [0.5701],\n",
       "        [0.5315],\n",
       "        [0.5699],\n",
       "        [0.6061],\n",
       "        [0.5533],\n",
       "        [0.5711],\n",
       "        [0.6204],\n",
       "        [0.5939],\n",
       "        [0.5412],\n",
       "        [0.6057],\n",
       "        [0.6028],\n",
       "        [0.5923],\n",
       "        [0.6074],\n",
       "        [0.6356],\n",
       "        [0.6556],\n",
       "        [0.6605],\n",
       "        [0.5795],\n",
       "        [0.6236],\n",
       "        [0.6417],\n",
       "        [0.6080],\n",
       "        [0.6226],\n",
       "        [0.6799],\n",
       "        [0.6597],\n",
       "        [0.7028],\n",
       "        [0.6185],\n",
       "        [0.6567],\n",
       "        [0.6517],\n",
       "        [0.6727],\n",
       "        [0.6272],\n",
       "        [0.6922],\n",
       "        [0.6710],\n",
       "        [0.7176],\n",
       "        [0.7189],\n",
       "        [0.6473],\n",
       "        [0.6736],\n",
       "        [0.7077],\n",
       "        [0.6894],\n",
       "        [0.7552],\n",
       "        [0.6609],\n",
       "        [0.7134],\n",
       "        [0.7229],\n",
       "        [0.6699],\n",
       "        [0.7439],\n",
       "        [0.7343],\n",
       "        [0.7355],\n",
       "        [0.7576],\n",
       "        [0.6833],\n",
       "        [0.7408],\n",
       "        [0.7384],\n",
       "        [0.7002],\n",
       "        [0.7451],\n",
       "        [0.7218],\n",
       "        [0.7719],\n",
       "        [0.7826]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dict = dict()\n",
    "\n",
    "for id, pred, gt in zip(train_concept.train_id, predictions.detach().numpy(), direction_label):\n",
    "    if id in prediction_dict.keys():\n",
    "        prediction_dict[id].append(pred)\n",
    "    else:\n",
    "        prediction_dict[id] = [pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in prediction_dict.keys():\n",
    "    prediction_dict[key] = np.mean(prediction_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 0.50363606,\n",
       " '2': 0.5249094,\n",
       " '4': 0.5223248,\n",
       " '5': 0.54297376,\n",
       " '6': 0.569166,\n",
       " '7': 0.5847024,\n",
       " '8': 0.5832562,\n",
       " '9': 0.6302744,\n",
       " '10': 0.61493224,\n",
       " '11': 0.6546005,\n",
       " '12': 0.64990735,\n",
       " '13': 0.68540365,\n",
       " '14': 0.6946336,\n",
       " '15': 0.6990939,\n",
       " '16': 0.72823846,\n",
       " '17': 0.7208233,\n",
       " '18': 0.7443426}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16100314b3ba5fe3ed1a4dcc36124eb3da3396790081b2a994ef9064dc4ea70e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
